num_steps: 2_000_000
batch_size: 128
N: 32
num_cosines: 64
ent_coef: 0  # You can use entropy loss as a regularizer.
kappa: 1.0
quantile_lr: 5.e-5
fraction_lr: 2.5e-9
memory_size: 1000000
gamma: 1
multi_step: 1
update_interval: 1
target_update_interval: 5_000
start_steps: 10_000
epsilon_train: 0.01
epsilon_eval: 0.001
epsilon_decay_steps: 25_000
double_q_learning: False
dueling_net: False
noisy_net: False
use_per: True
log_interval: 100
eval_interval: 1_000
num_eval_steps: 125000 
max_episode_steps: 27000
grad_cliping:
